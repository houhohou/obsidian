#openstack 
[[오픈스택 올인원 삼중화]]

- [b] 설치단계
1. chrony
2. percona
3. rabbitmq
4. pacemaker & corosync & pcsd
5. haproxy
6. memcached


# chrony
[[chrony]]
설치 & 백업 & conf 수정
```bash
#1

# 설치
apt install -y chrony  --fix-missing
# 백업
cp /etc/chrony/chrony.conf /etc/chrony/chrony.conf.bak
# 기본 pool 주석
sed -i 's/^pool/#pool/' /etc/chrony/chrony.conf
# Conf 파일 추가
cat >> /etc/chrony/chrony.conf << E0F
server time.bora.net iburst
server send.mx.cdnetworks.com iburst
allow 10.185.214.0/24
E0F
# 시간대 설정
timedatectl set-timezone Asia/Seoul
# 재시작
service chrony.service restart && systemctl enable chrony.service
# 방화벽 오픈
ufw allow 123/udp && ufw allow 323/udp

```
**현장가면 타임서버가 없어서, 1번서버가 주축이 되게 시간맞추면됨**


chrony 설치, 백업, conf수정, 재시작
```bash
# 바라볼 서버 2,3

# 설치
apt install -y chrony --fix-missing
# 백업
cp /etc/chrony/chrony.conf /etc/chrony/chrony.conf.bak
# 기본 pool 주석
sed -i 's/^pool/#pool/' /etc/chrony/chrony.conf

#conf에 바라볼 주소 설정
cat >> /etc/chrony/chrony.conf << E0F
server 10.185.214.230 iburst
E0F

# 재시작
service chrony.service restart && systemctl enable chrony.service
# 방화벽 오픈
ufw allow 123/udp && ufw allow 323/udp
```

## 동작확인
```bash
#1
chronyc -a makestep && chronyc sources
```


```bash
#2,3
chronyc -a makestep && chronyc sources
```

# 오픈스택 cli 설치
```bash
#1,2,3
apt install -y python3-openstackclient
```


# percona db 설정

[[perconaxtradb 억까리스트]]
[[perconaxtradb 유저 생성]]

```bash
#1,2,3
sudo apt update
sudo apt install -y wget gnupg2 lsb-release curl --fix-missing
wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb
sudo dpkg -i percona-release_latest.generic_all.deb
sudo apt update
sudo percona-release setup pxc80
sudo apt install -y percona-xtradb-cluster --fix-missing

systemctl stop mysql

ufw allow 3306
ufw allow 4444
ufw allow 4567
ufw allow 4568
```

all01 서버
```bash
#1
cp /etc/mysql/mysql.conf.d/mysqld.cnf /etc/mysql/mysql.conf.d/mysqld.cnf.bak

cat > /etc/mysql/mysql.conf.d/mysqld.cnf << E0F
[client]
socket=/var/run/mysqld/mysqld.sock
[mysqld]
server-id=1
datadir=/var/lib/mysql
socket=/var/run/mysqld/mysqld.sock
log-error=/var/log/mysql/error.log
pid-file=/var/run/mysqld/mysqld.pid
binlog_expire_logs_seconds=604800
wsrep_provider=/usr/lib/galera4/libgalera_smm.so
wsrep_cluster_address=gcomm://10.185.14.251,10.185.14.252,10.185.14.253
binlog_format=ROW
wsrep_slave_threads=8
wsrep_log_conflicts
innodb_autoinc_lock_mode=2
wsrep_cluster_name=pxc-cluster
wsrep_node_name=all01
wsrep_node_address=10.185.214.251
pxc_strict_mode=ENFORCING
wsrep_sst_method=xtrabackup-v2
pxc-encrypt-cluster-traffic=OFF
max_connections=20000
bind-address=0.0.0.0
E0F
```
all02 서버
```bash
#2
cp /etc/mysql/mysql.conf.d/mysqld.cnf /etc/mysql/mysql.conf.d/mysqld.cnf.bak

cat > /etc/mysql/mysql.conf.d/mysqld.cnf << E0F
[client]
socket=/var/run/mysqld/mysqld.sock
[mysqld]
server-id=1
datadir=/var/lib/mysql
socket=/var/run/mysqld/mysqld.sock
log-error=/var/log/mysql/error.log
pid-file=/var/run/mysqld/mysqld.pid
binlog_expire_logs_seconds=604800
wsrep_provider=/usr/lib/galera4/libgalera_smm.so
wsrep_cluster_address=gcomm://10.185.14.251,10.185.14.252,10.185.14.253
binlog_format=ROW
wsrep_slave_threads=8
wsrep_log_conflicts
innodb_autoinc_lock_mode=2
wsrep_cluster_name=pxc-cluster
wsrep_node_name=all02
wsrep_node_address=10.185.214.252
pxc_strict_mode=ENFORCING
wsrep_sst_method=xtrabackup-v2
pxc-encrypt-cluster-traffic=OFF
max_connections=20000
bind-address=0.0.0.0
E0F
```

all03서버
```bash
#3
cp /etc/mysql/mysql.conf.d/mysqld.cnf /etc/mysql/mysql.conf.d/mysqld.cnf.bak

cat > /etc/mysql/mysql.conf.d/mysqld.cnf << E0F
[client]
socket=/var/run/mysqld/mysqld.sock
[mysqld]
server-id=1
datadir=/var/lib/mysql
socket=/var/run/mysqld/mysqld.sock
log-error=/var/log/mysql/error.log
pid-file=/var/run/mysqld/mysqld.pid
binlog_expire_logs_seconds=604800
wsrep_provider=/usr/lib/galera4/libgalera_smm.so
wsrep_cluster_address=gcomm://10.185.14.251,10.185.14.252,10.185.14.253
binlog_format=ROW
wsrep_slave_threads=8
wsrep_log_conflicts
innodb_autoinc_lock_mode=2
wsrep_cluster_name=pxc-cluster
wsrep_node_name=all03
wsrep_node_address=10.185.214.253
pxc_strict_mode=ENFORCING
wsrep_sst_method=xtrabackup-v2
pxc-encrypt-cluster-traffic=OFF
max_connections=20000
bind-address=0.0.0.0
E0F
```

all01서버
```bash
#1
systemctl start mysql@bootstrap.service

mysql -u root -pcloud1234
show status like 'wsrep%';
```

all02서버
```/bin/bash
#2,3
systemctl start mysql
```

## 동기화 확인
```/bin/bash
#아무서버에서
mysql -u root -pcloud1234

create database ttt;

#다른서버
mysql -u root -cloud1234
show databases;

```


# rabbitmq
```bash
#1,2,3
# 설치
apt install -y rabbitmq-server
# 재시작
systemctl enable rabbitmq-server
service rabbitmq-server start

#rabbitmq관리 플러그인 시작
sudo rabbitmq-plugins enable rabbitmq_management
# 재시작
systemctl restart rabbitmq-server
```

```bash
# 수신 포트 확인 1,2,3
netstat -tunelp | grep 15672

tcp 0 0 0.0.0.0:15672 0.0.0.0:* LISTEN 119 1520547 435471/beam.smp
```
### 클러스터 설정
```bash
# 컨트롤러 1번에서 키값 전송해서 맞추기 1
scp /var/lib/rabbitmq/.erlang.cookie root@10.185.214.252:/var/lib/rabbitmq/
scp /var/lib/rabbitmq/.erlang.cookie root@10.185.214.253:/var/lib/rabbitmq/
```

```bash
#2,3
sed -i '/^127.0.1.1/s/^/# /' /etc/hosts

# 서비스 재시작 및 중지
systemctl restart rabbitmq-server
rabbitmqctl stop_app

# RAM 노드로 컨트롤러 1번에 모두 연결
rabbitmqctl join_cluster rabbit@all01 --ram

# 서비스 시작
rabbitmqctl start_app
```
## 동작확인
```bash
#1
rabbitmqctl cluster_status
```
### rabbitmq유저 생성
```bash
#1
# openstack 사용자 추가
rabbitmqctl add_user openstack cloud1234

# 사용자를 관리자 권한으로 설정
rabbitmqctl set_user_tags openstack administrator
rabbitmqctl set_permissions openstack ".*" ".*" ".*"

# 사용자 확인
rabbitmqctl list_users
```
# Pacemaker
```bash
# 1,2,3
# 패키지 설치
apt update && \
apt install -y pacemaker corosync pcs

# 재시작
systemctl start pcsd
systemctl enable --now pacemaker corosync pcsd

# 방화벽 생성
ufw allow 2224/tcp
ufw allow 3121/tcp
ufw allow 5403/tcp
ufw allow 5404/udp
ufw allow 5405/udp
ufw allow 21064/tcp
```
- [b] pcs 설정 
```bash
# 1
# 노드들 인증
pcs host auth con01 con02 con03 -u hacluster -p cloud1234

# 클러스터 setup 진행
pcs cluster setup hacluster con01 con02 con03

# 1,2,3
pcs cluster start
pcs cluster enable

# 1
# pcs 설정 변경
pcs property set stonith-enabled=false
pcs property set no-quorum-policy=ignore
pcs property set default-resource-stickiness="INFINITY" --force

#1 VIP 생성
pcs resource create vip ocf:heartbeat:IPaddr2 ip="10.185.214.64" cidr_netmask="24" op monitor timeout="30s" interval="20s" role="Slave" op monitor timeout="30s" interval="10s" role="Master"
```
- [b] 상태 확인 
```bash
#1,2,3
systemctl status pacemaker
systemctl status corosync
systemctl status pcsd
```
# haproxy
```bash
#1,2,3
# 설치
apt -y install haproxy --fix-missing

# conf백업
cp -p /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak

systemctl enable haproxy
systemctl restart haproxy
```
- [b] conf수정
```bash
cat >> /etc/haproxy/haproxy.conf << E0F
#-----------------------------
#       keystone
#-----------------------------
 listen keystone_public_internal_cluster
  bind 10.185.214.64:15000
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:5000 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:5000 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:5000 check inter 2000 rise 2 fall 5

#-----------------------------
#       glance
#-----------------------------
 listen glance_api_cluster
  bind 10.185.214.64:19292
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:9292 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:9292 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:9292 check inter 2000 rise 2 fall 5
#-----------------------------
#       placement
#-----------------------------
 listen placement_api_cluster
  bind 10.185.214.64:18778
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:8778 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:8778 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:8778 check inter 2000 rise 2 fall 5
#-----------------------------
#       nova cluster
#-----------------------------
 listen nova_cluster
  bind 10.185.214.64:18774
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:8774 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:8774 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:8774 check inter 2000 rise 2 fall 5

#-----------------------------
#       nova metadata
#-----------------------------
 listen nova_metadata_cluster
  bind 10.185.214.64:18775
  balance  source
  option  tcpka
  option  tcplog
  server con01 10.185.214.61:8775 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:8775 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:8775 check inter 2000 rise 2 fall 5

#-----------------------------
#       nova vnc proxy
#-----------------------------
 listen nova_vncproxy_cluster
  bind 10.185.214.64:16080
  balance  source
  option  tcpka
  option  tcplog
  server con01 10.185.214.61:6080 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:6080 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:6080 check inter 2000 rise 2 fall 5
#-----------------------------
#       horizon
#-----------------------------
listen int_horizon_cluster
  bind 10.185.214.64:443
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:80 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:80 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:80 check inter 2000 rise 2 fall 5

#-----------------------------
#       neutron
#-----------------------------
listen neutron_api_cluster
  bind 10.185.214.64:19696
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:9696 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:9696 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:9696 check inter 2000 rise 2 fall 5

#-----------------------------
#       cinder
#-----------------------------
 listen cinder_api_cluster
  bind 10.185.214.64:18776
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server con01 10.185.214.61:8776 check inter 2000 rise 2 fall 5
#  server con02 10.185.214.62:8776 check inter 2000 rise 2 fall 5
#  server con03 10.185.214.63:8776 check inter 2000 rise 2 fall 5

#-----------------------------
#       percona
#-----------------------------
 listen percona_cluster
  bind 10.185.214.64:13306
  mode tcp
  server con01 10.185.214.61:3306 check inter 2000 rise 2 fall 5
  server con02 10.185.214.62:3306 check inter 2000 rise 2 fall 5
  server con03 10.185.214.63:3306 check inter 2000 rise 2 fall 5
E0F
```

- [b] pcs 등록 
```bash
#1
# haproxy 서비스 등록 -> systemd:haproxy가 서비스를 관리
pcs resource create haproxy systemd:haproxy op monitor interval=10s --force

# vip resource가 먼저 실행이 되어있어야 haproxy 서비스가 실행될 수 있게 설정
pcs constraint order vip then haproxy

# vip와 haproxy resource가 동일한 node에서 실행될 수 있게 설정
pcs constraint colocation add vip with haproxy score=INFINITY

# haproxy resource cleanup 진행 (cleanup을 진행하면 resource가 실행되는 node가 다운되어도 바로 다른 node에서 resource가 실행됨)
pcs resource cleanup haproxy


# 1,2,3
systemctl enable haproxy
systemctl restart haproxy
```
- [b] 동작확인
```bash
1,2,3
pcs status
```


# memcached

```bash
# 1,2,3 
# 설치
apt install -y memcached python3-memcache
# 백업
cp /etc/memcached.conf /etc/memcached.conf.bak
# 각 controller ip로 변경
sudo sed -i 's/-l 127.0.0.1/-l 192.168.34.47/' /etc/memcached.conf

# 서비스 재시작
systemctl restart memcached.service
```